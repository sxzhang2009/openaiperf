# 技术实现指南

## 1. 快速开始

### 1.1 环境准备

#### 系统要求
```bash
# 最低系统要求
- CPU: 8核心以上
- 内存: 16GB以上
- 加速器: 支持CUDA/ROCm/oneAPI/Metal等计算后端
- 存储: 50GB可用空间
- 网络: 稳定的互联网连接

# 推荐配置
- CPU: 32核心以上
- 内存: 64GB以上
- 加速器: NVIDIA A100/H100、AMD MI300X、Intel Ponte Vecchio 或同等性能
- 存储: 500GB NVMe SSD
- 网络: 10GbE或更高
```

#### 软件依赖
```bash
# 基础软件
- Ubuntu 20.04+ 或 CentOS 8+
- Docker 20.10+
- 相应厂商的容器工具包
- Python 3.8+

# 可选软件
- NVIDIA Driver 525+ (NVIDIA GPU)
- AMD ROCm 6.0+ (AMD GPU)
- Intel oneAPI 2024+ (Intel GPU)
- Apple Metal 3.0+ (Apple Silicon)
```

### 1.2 安装配置

#### 安装OpenAIPerf CLI
```bash
# 使用pip安装
pip install openaiperf

# 或从源码安装
git clone https://github.com/openaiperf/openaiperf.git
cd openaiperf
pip install -e .

# 验证安装
ob --version
```

#### 配置认证
```bash
# 设置认证token
export OPENAIPERF_TOKEN="your_token_here"

# 或创建配置文件
mkdir -p ~/.openaiperf
cat > ~/.openaiperf/config.yaml << EOF
registry:
  url: "https://registry.openaiperf.org"
  auth_token: "${OPENAIPERF_TOKEN}"
EOF
```

## 2. 基础使用

### 2.1 系统探测

#### 自动探测
```bash
# 自动探测系统信息
ob init --auto

# 指定输出文件
ob init --output system.json

# 包含详细信息
ob init --detailed --include-power --include-topology
```

#### 手动配置
```json
// system.json 示例
{
  "hardware": {
    "cpu": {
      "model": "Intel Xeon 8480+",
      "cores": 56,
      "frequency": "3.8GHz"
    },
    "accelerators": [
      {
        "type": "gpu",
        "vendor": "nvidia",
        "name": "H100",
        "count": 8,
        "memory_gb": 80,
        "interconnect": "nvlink"
      }
    ],
    "memory": {
      "total_gb": 1024,
      "type": "DDR5"
    }
  },
  "software": {
    "driver": "550.54",
    "compute_backend": {
      "type": "cuda",
      "version": "12.4"
    }
  }
}
```

### 2.2 任务执行

#### 获取任务定义
```bash
# 列出可用任务
ob fetch --list

# 获取特定任务
ob fetch llm.generation --version 1.0.0

# 下载任务数据
ob fetch llm.generation --with-dataset --output-dir ./data
```

#### 执行评测
```bash
# 基础执行
ob run --task llm.generation --model llama3-8b

# Single Stream模式（单流，最低延迟）
ob run --task llm.generation \
  --model llama3-8b \
  --scenario single_stream \
  --backend vllm

# Offline模式（离线，最大吞吐量）
ob run --task llm.generation \
  --model llama3-8b \
  --scenario offline \
  --batch-size 32 \
  --backend vllm

# Server模式（服务器，平衡延迟和吞吐量）
ob run --task llm.generation \
  --model llama3-8b \
  --scenario server \
  --qps-target 100 \
  --latency-sla 100 \
  --backend vllm

# Multi Stream模式（多流，多用户并发）
ob run --task llm.generation \
  --model llama3-8b \
  --scenario multi_stream \
  --stream-count 10 \
  --backend vllm

# Interactive模式（交互式，流式生成）
ob run --task llm.generation \
  --model llama3-8b \
  --scenario interactive \
  --streaming true \
  --backend vllm

# 自定义配置
ob run --task llm.generation \
  --model llama3-8b \
  --backend vllm \
  --scenario server \
  --qps-target 100 \
  --repeats 3 \
  --subset-seed 1234
```

### 2.3 结果管理

#### 打包结果
```bash
# 打包当前结果
ob pack --output result.tgz

# 包含所有文件
ob pack --include-logs --include-configs --output result.tgz

# 压缩打包
ob pack --compress gzip --output result.tgz
```

#### 提交结果
```bash
# 提交到平台
ob submit --artifact result.tgz

# 指定标签
ob submit --artifact result.tgz --tags "llama3,vllm,server"

# 添加描述
ob submit --artifact result.tgz --description "Llama3-8B Server场景评测"
```

## 3. 高级配置

### 3.1 自定义任务

#### 创建任务定义
```yaml
# custom_task.yaml
name: custom.llm.generation
version: 1.0.0
description: "自定义LLM生成任务"

datasets:
  eval:
    source: "https://example.com/dataset"
    sha256: "abc123..."
    format: "jsonl"
    license: "MIT"

metrics:
  quality:
    - name: "perplexity"
      target: 15.0
      lower_is_better: true
  
  performance:
    - name: "tokens_per_second"
      unit: "tokens/sec"
      higher_is_better: true
    
    - name: "latency_p99"
      unit: "ms"
      lower_is_better: true

scenarios:
  - "server"
  - "offline"

input_profile:
  max_length: 2048
  temperature: 0.7
  top_p: 0.9

warmup:
  batches: 50
  samples: 100
```

#### 注册自定义任务
```bash
# 注册任务
ob task register custom_task.yaml

# 验证任务
ob task validate custom_task.yaml

# 列出自定义任务
ob task list --custom
```

### 3.2 自定义后端

#### 实现后端接口
```python
# custom_backend.py
from openaiperf.backend import BackendInterface
import torch

class CustomBackend(BackendInterface):
    def __init__(self, config):
        self.config = config
        self.model = None
        self.tokenizer = None
    
    def prepare(self, model_config, system_config):
        """模型准备阶段"""
        # 加载模型
        self.model = torch.load(model_config['path'])
        self.tokenizer = self.load_tokenizer(model_config['tokenizer_path'])
        
        # 移动到GPU
        if torch.cuda.is_available():
            self.model = self.model.cuda()
        
        # 设置为评估模式
        self.model.eval()
        
        return True
    
    def run_batch(self, inputs, config):
        """批量推理执行"""
        # 预处理输入
        tokens = self.tokenizer(inputs, return_tensors='pt')
        
        # 移动到GPU
        if torch.cuda.is_available():
            tokens = {k: v.cuda() for k, v in tokens.items()}
        
        # 执行推理
        with torch.no_grad():
            outputs = self.model.generate(
                **tokens,
                max_length=config.get('max_length', 100),
                temperature=config.get('temperature', 0.7),
                do_sample=True
            )
        
        # 后处理输出
        results = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        
        return {
            'outputs': results,
            'metrics': {
                'latency_ms': 100.0,  # 示例值
                'tokens_generated': len(outputs[0])
            }
        }
    
    def teardown(self):
        """资源清理"""
        if self.model is not None:
            del self.model
        if self.tokenizer is not None:
            del self.tokenizer
        torch.cuda.empty_cache()
```

#### 注册自定义后端
```bash
# 注册后端
ob backend register custom_backend.py --name custom_backend

# 使用自定义后端
ob run --task llm.generation --backend custom_backend
```

### 3.3 容器化部署

#### 创建Dockerfile
```dockerfile
# Dockerfile
FROM nvidia/cuda:12.4-devel-ubuntu22.04

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# 设置工作目录
WORKDIR /workspace

# 复制依赖文件
COPY requirements.txt .
COPY custom_backend.py .

# 安装Python依赖
RUN pip install -r requirements.txt
RUN pip install openaiperf

# 设置环境变量
ENV OPENAIPERF_REGISTRY_URL=https://registry.openaiperf.org
ENV OPENAIPERF_CACHE_DIR=/cache
ENV OPENAIPERF_OUTPUT_DIR=/output

# 创建必要的目录
RUN mkdir -p /cache /output

# 设置入口点
ENTRYPOINT ["ob"]
```

#### 构建和运行容器
```bash
# 构建镜像
docker build -t openaiperf-custom .

# 运行容器
docker run --gpus all \
  -v $(pwd)/data:/workspace/data \
  -v $(pwd)/output:/output \
  -e OPENAIPERF_TOKEN=$OPENAIPERF_TOKEN \
  openaiperf-custom run \
  --task llm.generation \
  --backend custom_backend
```

## 4. 监控和调试

### 4.1 实时监控

#### 系统监控
```bash
# 启动系统监控
ob monitor --system --interval 1s --output system_metrics.csv

# 监控特定指标
ob monitor --metrics cpu,memory,gpu,power --interval 5s

# 实时显示
ob monitor --live --metrics cpu,gpu
```

#### 任务监控
```bash
# 监控任务执行
ob run --task llm.generation --monitor --live

# 保存监控数据
ob run --task llm.generation --monitor --output monitoring.json

# 自定义监控配置
ob run --task llm.generation \
  --monitor \
  --monitor-interval 1s \
  --monitor-metrics latency,throughput,memory
```

### 4.2 日志分析

#### 查看日志
```bash
# 查看系统日志
ob logs system --tail 100

# 查看任务日志
ob logs run --tail 100

# 搜索日志
ob logs --grep "error" --since "1h ago"

# 导出日志
ob logs --export logs.json --since "2024-01-15"
```

#### 日志分析工具
```python
# log_analyzer.py
import json
import pandas as pd
from datetime import datetime

def analyze_logs(log_file):
    """分析日志文件"""
    with open(log_file, 'r') as f:
        logs = [json.loads(line) for line in f]
    
    # 转换为DataFrame
    df = pd.DataFrame(logs)
    
    # 时间戳转换
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # 分析性能指标
    if 'latency_ms' in df.columns:
        print(f"平均延迟: {df['latency_ms'].mean():.2f}ms")
        print(f"P99延迟: {df['latency_ms'].quantile(0.99):.2f}ms")
    
    # 分析错误
    errors = df[df.get('level', '') == 'ERROR']
    if not errors.empty:
        print(f"错误数量: {len(errors)}")
        print("错误类型:")
        print(errors['message'].value_counts())
    
    return df

# 使用示例
if __name__ == "__main__":
    df = analyze_logs("run.log")
    print("日志分析完成")
```

### 4.3 性能调优

#### 性能分析
```bash
# 性能分析模式
ob run --task llm.generation --profile --output profile.json

# 详细性能分析
ob run --task llm.generation \
  --profile \
  --profile-metrics cpu,gpu,memory,network \
  --profile-interval 100ms

# 生成性能报告
ob profile --input profile.json --output report.html
```

#### 优化建议
```python
# performance_advisor.py
def analyze_performance(profile_data):
    """分析性能数据并提供优化建议"""
    suggestions = []
    
    # 分析GPU利用率
    gpu_util = profile_data.get('gpu_utilization', 0)
    if gpu_util < 80:
        suggestions.append({
            'type': 'gpu_utilization',
            'issue': f'GPU利用率较低: {gpu_util:.1f}%',
            'suggestion': '考虑增加批量大小或优化数据加载'
        })
    
    # 分析内存使用
    memory_usage = profile_data.get('memory_usage', 0)
    if memory_usage > 90:
        suggestions.append({
            'type': 'memory_usage',
            'issue': f'内存使用率过高: {memory_usage:.1f}%',
            'suggestion': '考虑减少批量大小或使用梯度检查点'
        })
    
    # 分析延迟分布
    latency_p99 = profile_data.get('latency_p99', 0)
    if latency_p99 > 1000:
        suggestions.append({
            'type': 'latency',
            'issue': f'P99延迟过高: {latency_p99:.1f}ms',
            'suggestion': '考虑模型优化或硬件升级'
        })
    
    return suggestions
```

## 5. 故障排除

### 5.1 常见问题

#### 安装问题
```bash
# 问题：CUDA版本不匹配
# 解决方案：
nvidia-smi  # 检查驱动版本
nvcc --version  # 检查CUDA版本
pip install torch==2.4.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html

# 问题：权限不足
# 解决方案：
sudo usermod -a -G docker $USER
newgrp docker
```

#### 运行问题
```bash
# 问题：GPU内存不足
# 解决方案：
ob run --task llm.generation --batch-size 1 --max-memory 8GB

# 问题：网络连接失败
# 解决方案：
ob config --registry-url https://registry.openaiperf.org
ob config --timeout 300

# 问题：任务执行失败
# 解决方案：
ob run --task llm.generation --debug --verbose
ob logs --grep "error" --since "10m ago"
```

### 5.2 调试技巧

#### 启用调试模式
```bash
# 设置调试级别
export OPENAIPERF_LOG_LEVEL=DEBUG

# 启用详细输出
ob run --task llm.generation --verbose --debug

# 保存调试信息
ob run --task llm.generation --debug --output debug_info.json
```

#### 环境检查
```bash
# 检查环境配置
ob doctor

# 检查系统兼容性
ob check --system

# 检查任务兼容性
ob check --task llm.generation
```

### 5.3 获取帮助

#### 文档和资源
```bash
# 查看帮助
ob --help
ob run --help

# 查看示例
ob examples --list
ob examples --show llm.generation

# 查看文档
ob docs --open
```

#### 社区支持
```bash
# 提交问题
ob issue --create --title "GPU内存不足" --description "详细描述..."

# 查看已知问题
ob issue --list

# 获取社区支持
ob support --channel discord
```
