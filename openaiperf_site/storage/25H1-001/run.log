2025-01-04T10:29:40Z [INFO] Starting OpenAIPerf evaluation...
2025-01-04T10:29:40Z [INFO] Configuration loaded: llm.generation task, server scenario
2025-01-04T10:29:41Z [INFO] Initializing pytorch engine...
2025-01-04T10:29:43Z [INFO] Loading model: llama3-8b
2025-01-04T10:29:45Z [INFO] Model loaded successfully, dtype: fp8
2025-01-04T10:29:45Z [INFO] Quantization: AWQ enabled
2025-01-04T10:29:46Z [INFO] Starting inference server
2025-01-04T10:29:47Z [INFO] Beginning evaluation run (3 repeats)
2025-01-04T10:29:50Z [METRIC] Repeat 1/3 - Throughput: 320.5 req/s
2025-01-04T10:29:53Z [METRIC] Repeat 2/3 - Throughput: 321.5 req/s
2025-01-04T10:29:56Z [METRIC] Repeat 3/3 - Throughput: 320.0 req/s
2025-01-04T10:29:56Z [RESULT] Final metrics - Throughput: 320.5 req/s, P99 Latency: 95.1ms
2025-01-04T10:29:56Z [RESULT] Quality score: 76.4/100
2025-01-04T10:29:57Z [INFO] Evaluation completed successfully