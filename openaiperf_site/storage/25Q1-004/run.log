2025-01-04T10:29:40Z [INFO] Starting OpenAIPerf evaluation...
2025-01-04T10:29:40Z [INFO] Configuration loaded: llm.generation task, offline scenario
2025-01-04T10:29:41Z [INFO] Initializing onnxruntime engine...
2025-01-04T10:29:43Z [INFO] Loading model: phi-3-mini
2025-01-04T10:29:45Z [INFO] Model loaded successfully, dtype: fp8
2025-01-04T10:29:45Z [INFO] Quantization: AWQ enabled
2025-01-04T10:29:46Z [INFO] Starting inference server
2025-01-04T10:29:47Z [INFO] Beginning evaluation run (3 repeats)
2025-01-04T10:29:50Z [METRIC] Repeat 1/3 - Throughput: 890.3 req/s
2025-01-04T10:29:53Z [METRIC] Repeat 2/3 - Throughput: 891.3 req/s
2025-01-04T10:29:56Z [METRIC] Repeat 3/3 - Throughput: 889.8 req/s
2025-01-04T10:29:56Z [RESULT] Final metrics - Throughput: 890.3 req/s, P99 Latency: 28.7ms
2025-01-04T10:29:56Z [RESULT] Quality score: 74.2/100
2025-01-04T10:29:57Z [INFO] Evaluation completed successfully